{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "from datasets.mix_music import MusicDataset,MixMusicDataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import os\n",
    "from torch import optim\n",
    "from utils.io_utils import create_directory_if_not_exists\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import subprocess\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 2\n",
    "EXP_NAME = \"train_model\"\n",
    "INPUT_CHANNEL = 1\n",
    "OUTPUT_CHANNEL = 2\n",
    "INPUT_SHAPE = (INPUT_CHANNEL,10000)\n",
    "OUTPUT_SHAPE = (OUTPUT_CHANNEL,10000)\n",
    "MAX_DATA_NUM = 50\n",
    "MAX_CROP_SECOND = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open tensorboard: 127.0.0.1:6677\n",
      "input size torch.Size([2, 1, 10000])\n",
      "output size torch.Size([2, 2, 10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_path = f'save/{EXP_NAME}/'\n",
    "create_directory_if_not_exists(model_path)\n",
    "\n",
    "log_path = f'logs/tensorboard/{EXP_NAME}/'\n",
    "create_directory_if_not_exists(log_path)\n",
    "writer = SummaryWriter(log_dir=log_path)\n",
    "\n",
    "process = subprocess.Popen(f'tensorboard --logdir={log_path} --port=6677 --bind_all', shell=True)\n",
    "# debug_print(f\"open tensorboard, cmd: tensorboard --logdir={log_path}\")\n",
    "print(f\"open tensorboard: 127.0.0.1:6677\")\n",
    "\n",
    "input_sample = torch.randn((2,) + INPUT_SHAPE).to(DEVICE)\n",
    "print(\"input size\",input_sample.shape)\n",
    "output_sample = torch.randn((2,) + OUTPUT_SHAPE).to(DEVICE)\n",
    "print(\"output size\",output_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = MusicDataset(basicSize=OUTPUT_CHANNEL,maxDataNum=MAX_DATA_NUM,max_CropSecond=MAX_CROP_SECOND)\n",
    "dataloader = MixMusicDataLoader(dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class AdaptivePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AdaptivePositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Initialize learnable positional embeddings\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, 1, d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # Expand learnable positional embeddings to match input sequence length\n",
    "        position_embeddings = self.position_embeddings.expand(batch_size, seq_len, -1)\n",
    "\n",
    "        return position_embeddings\n",
    "        \n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, nhead=8, num_layers=3, d_model=512):\n",
    "        super(MyTransformer, self).__init__()\n",
    "        self.src_embedding = nn.Linear(input_channels, d_model)\n",
    "        self.tgt_embedding = nn.Linear(output_channels, d_model)\n",
    "        self.pos_encoder = AdaptivePositionalEncoding(d_model)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead), num_layers=num_layers)\n",
    "        self.head = nn.Linear(d_model,output_channels)\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "    def forward(self, src,tgt):\n",
    "        src = src.permute(2, 0, 1)\n",
    "        tgt = tgt.permute(2, 0, 1)\n",
    "        \n",
    "        embedded_src = self.src_embedding(src)\n",
    "        embedded_src = self.pos_encoder(embedded_src)\n",
    "        encoded_src = self.transformer_encoder(embedded_src)\n",
    "     \n",
    "\n",
    "        embedded_tgt = self.tgt_embedding(tgt)\n",
    "        embedded_tgt = self.pos_encoder(embedded_tgt)\n",
    "        \n",
    "        decoded_tgt = self.transformer_decoder(embedded_tgt, encoded_src)\n",
    "       \n",
    "        out = self.head(decoded_tgt)\n",
    "       \n",
    "        out = out.permute(1, 2, 0)\n",
    "        return out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.lib.display import Audio\n",
    "\n",
    "\n",
    "def inference(model, src, out_channel):\n",
    "    model.eval()\n",
    "    start_tensor = torch.zeros(src.size(0), out_channel, 1).to(src.device)\n",
    "    generated_sequence = []\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(src.size(2)),desc='Inference',position=0, dynamic_ncols=True,leave=False):\n",
    "\n",
    "            output = model(src, start_tensor)\n",
    "\n",
    "            next_token_value = output[:, :, -1]\n",
    "\n",
    "            generated_sequence.append(next_token_value)\n",
    "\n",
    "            next_token_tensor = torch.unsqueeze(next_token_value, dim=2)\n",
    "\n",
    "            start_tensor = torch.cat((start_tensor, next_token_tensor), dim=2)\n",
    "\n",
    "    return generated_sequence\n",
    "    \n",
    "def train_model(model, dataloader, optimizer, epoch,writer,model_name):\n",
    "    model.train()  # 模型训练\n",
    "    total_loss = 0.0\n",
    "    loss = []\n",
    "\n",
    "    loop = tqdm(enumerate(dataloader) ,total=len(dataloader),position=0)\n",
    "    for batch_index, (data_waveform,target_waveforms,sample_rate) in loop:\n",
    "        src, target= data_waveform.to(DEVICE), target_waveforms.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()  # 梯度初始化为0\n",
    "\n",
    "        output = model(src,target)  # 训练后的结果\n",
    "\n",
    "        sep_loss = torch.nn.functional.mse_loss(torch.mean(output,dim=1,keepdim=True), src)\n",
    "\n",
    "        total_loss = sep_loss\n",
    "        loss.append(total_loss.item())\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()  # 参数优化\n",
    "        \n",
    "        loop.set_description(f'Train Epoch [{epoch}/{EPOCHS}]')\n",
    "        loop.set_postfix(loss = total_loss.item())\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_loss = total_loss.item() / len(dataloader)\n",
    "    # mean_loss = sum(loss) / len(loss)\n",
    "    writer.add_scalar(f'Train/Loss/{model_name}', avg_loss, epoch)\n",
    "def test_Model(model, dataloader, epoch, writer, minLoss, model_path, model_name):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loop = tqdm(enumerate(dataloader) ,total=len(dataloader),position=0)\n",
    "        for batch_index, (data_waveform, target_waveforms, sample_rate) in loop:\n",
    "            src, target = data_waveform.to(DEVICE), target_waveforms.to(DEVICE)\n",
    "\n",
    "\n",
    "            generated_sequence = inference(model, src, out_channel=target.size(1))\n",
    "\n",
    "            generated_sequence_tensor = torch.stack(generated_sequence, dim=2).to(DEVICE)\n",
    "            if epoch%20 == 0 and batch_index == 0:\n",
    "                tqdm.write(\"input\")\n",
    "                display(Audio(src[0][0].cpu().numpy(), rate=sample_rate))\n",
    "                for i in range(generated_sequence_tensor[0].size(0)):\n",
    "                    tqdm.write(f\"output{i}\")\n",
    "                    display(Audio(generated_sequence_tensor[0][i].cpu().numpy(), rate=sample_rate))\n",
    "            # 计算损失\n",
    "            sep_loss = torch.nn.functional.mse_loss(torch.mean(generated_sequence_tensor,dim=1,keepdim=True), src)\n",
    "\n",
    "            total_loss += sep_loss\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            loop.set_description(f'Test Epoch [{epoch}/{EPOCHS}]')\n",
    "            loop.set_postfix(loss = avg_loss.item())\n",
    "\n",
    "        writer.add_scalar(f'Test/Loss/{model_name}', avg_loss, epoch)\n",
    "\n",
    "        if minLoss > avg_loss:\n",
    "            model_name = os.path.join(model_path, f'{model_name}.ckpt')\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "            print(f'save model to {model_name}')\n",
    "            return avg_loss\n",
    "\n",
    "        return minLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(MODEL_NAME='LMSN'):\n",
    "    model = MyTransformer(input_channels=INPUT_CHANNEL,output_channels=OUTPUT_CHANNEL,d_model=8,nhead=OUTPUT_CHANNEL)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    writer.add_graph(model, (input_sample,output_sample))\n",
    "    \n",
    "\n",
    "    optimizer= optim.Adam(model.parameters())\n",
    "\n",
    "    print(f\"train {MODEL_NAME}\")\n",
    "    print(\"------------------\")\n",
    "\n",
    "    loss = 0.0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_model(model, dataloader, optimizer, epoch,writer,MODEL_NAME)\n",
    "        loss=test_Model(model, dataloader,epoch,writer,loss,model_path,MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/murinj/project/src/Layered-Music-Separation/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "TensorBoard 2.16.2 at http://DESKTOP-UQPIEF5.:6677/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train LMSN\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch [1/1000]:  45%|█████████████████████████████████████████████████████████████████████████▋                                                                                        | 5/11 [01:51<02:07, 21.17s/it, loss=0.259]"
     ]
    }
   ],
   "source": [
    "exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
